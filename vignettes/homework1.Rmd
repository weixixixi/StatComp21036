---
title: "homework1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

1st

1. Text

Calculate to regression equation and test it.
```{r}
x<-c(0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.20, 0.21, 0.23)
y<-c(42.0, 43.5, 45.0, 45.5, 45.0, 47.5, 49.0, 53.0, 50.0, 55.0, 55.0, 60.0)
lm.sol<-lm(y ~ 1+x)
summary(lm.sol)
```

2. Figure

Describe the relationship between two variables using scatter diagram.
```{r}
Height<-c(56.5, 65.3, 64.3, 56.3, 59.8, 66.5, 51.3, 62.5, 62.8, 69.0, 63.5, 67.0, 57.3, 62.5, 59.0, 72.0, 64.8, 57.5, 66.5)
Weight<-c( 84.0, 98.0, 90.0, 77.0, 84.5, 112.0, 50.5, 112.5, 102.5, 112.5, 102.5, 133.0, 83.0, 84.0, 99.5, 150.0, 128.0, 85.0, 112.0)
plot(Weight~Height)
```

3. Table

Show the estimation results of parameters.
```{r}
library(knitr)
a <- c(0.3901401, 0.38509514, 0.3951850, 0.1016328, 0.08827257, 0.1149931)
m <- matrix(a,nrow=2,ncol=3,dimnames=list(c("a","b"),c("Estimate","Left","Right")))
knitr::kable(m, caption="CI")
```

2nd

3.4: 

1.Develop an algorithm to generate random samples from a $Rayleigh(\sigma)$ distribution.
(1)Calculate cdf:$$F(x)=1-{\rm exp}\left\{-\frac{x^2}{2\sigma^2}\right\}.$$
(2)Generate the random number $U\sim U(0,1)$.
(3)Return $\sqrt{-2\sigma^2{\rm ln}(u)}$.
2.Generate $Rayleigh(\sigma)$ samples for several choices of $\sigma > 0$.
```{r}
#σ=1
n <- 100
u <- runif(n)
x1 <- sqrt(-2*log(u))
x1
#σ=2
x2 <- sqrt(-8*log(u))
x2
#σ=0.5
x3 <- sqrt(-0.5*log(u))
x3
#σ=0.1
x4 <- sqrt(-0.02*log(u))
x4
```
3.Check the histogram to see if the mode of the generated samples is close to the theoretical mode $\sigma$.
```{r}
#σ=1
hist(x1, prob = TRUE, breaks = 20, col="light blue", main = expression(f(x)==x*exp(-x^2/2)), axes = TRUE, cex.main = 0.75)
y <- seq(0, 3.5, .01)
lines(y, y*exp(-y^2/2), col="red")
#σ=2
hist(x2, prob = TRUE, breaks = 20, col="light blue", main = expression(f(x)==(x/4)*exp(-x^2/8)), axes = TRUE, cex.main = 0.75)
y <- seq(0, 7, .01)
lines(y, (y/4)*exp(-y^2/8), col="red")
#σ=0.5
hist(x3, prob = TRUE, breaks = 20, col="light blue", main = expression(f(x)==4*x*exp(-2*x^2)), axes = TRUE, cex.main = 0.75)
y <- seq(0, 1.5, .01)
lines(y, 4*y*exp(-2*y^2), col="red")
#σ=0.1
hist(x4, prob = TRUE, breaks = 20, col="light blue", main = expression(f(x)==100*x*exp(-50*x^2)), axes = TRUE, cex.main = 0.75)
y <- seq(0, 3.5, .01)
lines(y, 100*y*exp(-50*y^2), col="red")
```

Conclusion:

When $\sigma$ takes different values, the above four diagrams can be obtained. The close distance between the graph and the histogram illustrates that the mode of the generated samples is close to the theoretical mode $\sigma$.

\
3.11: 

1.When $p_1=0.75$, generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0, 1)$ and $N(3, 1)$ distributions with mixing probabilities $p_1$ and $p_2 = 1 − p_1$.

Algorithm:

(1)Generate the random number $U \sim U(0,1)$.

(2)If $U<p_1$, take a sample $n_1$ from $N(0,1)$ otherwise take it from $N(3,1)$.

(3)Repeat the above steps for 1000 times.

```{r}
f_sample <- function(p1){
n <- c(1:1000)
i <- 1
while (i<=1000) {
  u <- runif(1)
  if(u < p1)
    {n[i] <- rnorm(1,0,1)} else
    {n[i] <- rnorm(1,3,1)}
  i=i+1
}
n}
n <- f_sample(0.75)
```

2.Graph the histogram of the sample with density superimposed.
```{r}
hist(n, breaks=50, prob = TRUE)
```

3.Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal.
```{r}
#p_1=0.9
n1 <- f_sample(0.9)
hist(n1, breaks=50, col="red")

#p_1=0.5
n2 <- f_sample(0.5)
hist(n2, breaks=50, col="orange")

#p_1=0.1
n3 <- f_sample(0.1)
hist(n3, breaks=50, col="green")
plot.ecdf(n)
text(1.8,0.8,expression(p1==0.75))
y <- seq(0.001, 1, by = 0.001)
lines(sort(n1), y, col="red")
text(1,0.95,expression(p1==0.9))
lines(sort(n2), y, col="orange")
text(1.8,0.5,expression(p1==0.5))
lines(sort(n3), y, col="green")
text(2,0.1,expression(p1==0.1))
```
```{r}

hist(f_sample(0.3), breaks=50, prob = TRUE)
hist(f_sample(0.7), breaks=50, prob = TRUE)
hist(f_sample(0.45), breaks=50, prob = TRUE)
hist(f_sample(0.55), breaks=50, prob = TRUE)

```

4.Make a conjecture about the values of p1 that produce bimodal mixtures.

From the above diagrams, we can see the empirical distribution of the mixture appears to be bimodal when the value of $p_1$ is around 0.5.

\
3.20:

1.Write a program to simulate a compound $P(λ)–Ga$ process ($Y$ has a Gamma distribution).

$$Y\sim Ga(\alpha,\beta)$$

Algorithm:

(1)$X=0$, $I=1$.

(2)Generate a random number $N\sim P(\lambda t)$

(3)Generate a random number $Y\sim Ga(\alpha,\beta)$.

(4)$X=X+Y$, $I=I+1$.

(5)If $I\geq N$, Stop. Otherwise, repeat Step(3).

(6)Return $X$.

Then simulate the compound Poisson process and let $t=10$, the parameter $\lambda=6$, $\alpha=1$, $\beta=2$:
```{r}
f_poisson <- function(t, lambda, alpha, beta){
X <- 0
I <- 1
N <- rpois(1, lambda*t)
while (I <= N) {
  u <- rgamma(1, alpha, beta)
  X = X + u
  I = I + 1
}
X}
round(f_poisson(10,6,1,2),4)
```

2. Estimate the mean and the variance of $X(10)$ for several choices of the parameters and compare with the theoretical values.

When $t=10$, theoretical mean and variance can be calculated:

$$E(X(t))=\lambda tE(Y_1)=\lambda\times 10\times \frac{\alpha}{\beta}=\frac{10\alpha\lambda}{\beta}$$
$$Var(X(t))=\lambda tE(Y_1^2)=\lambda\times 10\times \frac{\alpha+\alpha^2}{\beta^2}=\frac{10(\alpha+\alpha^2)\lambda}{\beta^2}$$
Repeat the above algorithm for $N$ times to take mean and variance of $X(10)$.

(a) When $\lambda=1$, $\alpha=1$, $\beta=1$, the theoretical values of mean and variance are 10, 20, respectively.
```{r}
f_meanvar <- function(N, lambda, alpha, beta){
  m <- c(1:N)
  for(i in 1:N){
    m[i] <- f_poisson(10, lambda, alpha, beta)
  }
  a1 <- mean(m)
  b1 <- var(m)
  a2 <- 10*alpha*lambda/beta
  b2 <- 10*(alpha+alpha^2)*lambda/beta^2
  e1 <- abs(a1-a2)/a2
  e2 <- abs(b1-b2)/b2
  e2
  d <- c(a1, a2, e1, b1, b2, e2)
  d
}
r1 <- round(f_meanvar(1000,1,1,1),4)
r1
```

(b) When $\lambda=1$, $\alpha=1$, $\beta=2$, the theoretical values of mean and variance are 5, 5, respectively.
```{r}
r2 <- round(f_meanvar(1000,1,1,2),4)
r2
```

(c) When $\lambda=1$, $\alpha=2$, $\beta=1$, the theoretical values of mean and variance are 20, 60, respectively.
```{r}
r3 <- round(f_meanvar(1000,1,2,1),4)
r3
```

(d) When $\lambda=1$, $\alpha=2$, $\beta=2$, the theoretical values of mean and variance are 10, 15, respectively.
```{r}
r4 <- round(f_meanvar(1000,1,2,2),4)
r4
```

(e) When $\lambda=2$, $\alpha=1$, $\beta=1$, the theoretical values of mean and variance are 20, 40, respectively.
```{r}
r5 <- round(f_meanvar(1000,2,1,1),4)
r5
```

(f) When $\lambda=3$, $\alpha=1$, $\beta=1$, the theoretical values of mean and variance are 10, 10, respectively.
```{r}
r6 <- round(f_meanvar(1000,2,1,2),4)
r6
```

(g) When $\lambda=2$, $\alpha=2$, $\beta=1$, the theoretical values of mean and variance are 40, 120, respectively.
```{r}
r7 <- round(f_meanvar(1000,2,2,1),4)
r7
```


(h) When $\lambda=2$, $\alpha=2$, $\beta=2$, the theoretical values of mean and variance are 20, 30, respectively.
```{r}
r8 <- round(f_meanvar(1000,2,2,2),4)
r8
```

Estimations of the mean and the variance and their comparison between estimations and theoretical values(errors) of $X(10)$ for several choices of the parameters$(\lambda, \alpha, \beta)$ are listed below:
```{r}
a <- c(r1, r2, r3, r4, r5, r6, r7, r8)
m <- matrix(a, nrow = 8, ncol = 6, byrow = TRUE, dimnames = list(c("(1,1,1)", "(1,1,2)", "(1,2,1)", "(1,2,2)", "(2,1,1)", "(2,1,2)", "(2,2,1)", "(2,2,2)"),c("Simutation m", "Theory m", "Error m", "Simutation v", "Theory v", "Error v")))
knitr::kable(m)
```

The errors of means and variances are all less than 0.1, so we can make a conclusion from this form that the estimations of mean and variance values are close to the theoretical values.

3rd

5.4: Beta(3, 3) pdf is:$$h(x)=30x^2(1-x)^2,$$
$$g(y)=30xy^2(1-y)^2,Y\sim N(0,x).$$
Beta(3, 3) cdf is:$$F(x)=\int_0^xh(t)dt=E[(g(y))]=E_Y[30xy^2(1-y)^2],Y\sim N(0,x).$$
Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf and use the function to estimate $F(x)$ for $x = 0.1, 0.2,..., 0.9$. Compare the estimates with the values returned by the $pbeta$ function in R.
```{r}
f_1 <- function(x){
  m <- 1e4
  y <- runif(m, min = 0, max = x)
  F_hat <- mean(y^2*(1-y)^2)*30*x
  F_hat
}
for(i in 1:9){
  a <- f_1(i/10)
  b <- pbeta(i/10,3,3)
  err <- abs((a-b)/b)
  d <- c(a, b, err)
  print(round(d,4))
}
```
Conclusion: We could get the result of small difference(<0.05) between MC method and $pbeta$ function in R.

\
5.9:
Implement a function to generate samples from a Rayleigh(σ) distribution, using antithetic variables.
(1)Calculate cdf:$$F(x)=1-{\rm exp}\left\{-\frac{x^2}{2\sigma^2}\right\}.$$\
(2)Generate the random number $U\sim U(0,1)$.\
(3)Return $\sqrt{-2\sigma^2{\rm ln}(u)}$.\
Generate $Rayleigh(\sigma)$ samples for several choices of $\sigma > 0$.\
```{r}
#σ=1
n <- 1000
u <- runif(n/2)
#σ=1
x1 <- sqrt(-2*log(u))
#σ=2
x2 <- sqrt(-8*log(u))
#σ=0.5
x3 <- sqrt(-0.5*log(u))
#σ=0.1
x4 <- sqrt(-0.02*log(u))
#σ=1
y1 <- sqrt(-2*log(1-u))
#σ=2
y2 <- sqrt(-8*log(1-u))
#σ=0.5
y3 <- sqrt(-0.5*log(1-u))
#σ=0.1
y4 <- sqrt(-0.02*log(1-u))
u <- runif(n/2)
#σ=1
z1 <- sqrt(-2*log(u))
#σ=2
z2 <- sqrt(-8*log(u))
#σ=0.5
z3 <- sqrt(-0.5*log(u))
#σ=0.1
z4 <- sqrt(-0.02*log(u))
scales::percent((var((x1+z1)/2)-var((x1+y1)/2))/var((x1+z1)/2),0.01)
scales::percent((var((x2+z2)/2)-var((x2+y2)/2))/var((x2+z2)/2),0.01)
scales::percent((var((x3+z3)/2)-var((x3+y3)/2))/var((x3+z3)/2),0.01)
scales::percent((var((x4+z4)/2)-var((x4+y4)/2))/var((x4+z4)/2),0.01)
```
Conclusion: We could see using antithetic variables can represent a more than 90% reduction in variance, compared with simple method.

\
5.13:
Find two importance functions f1 and f2 that are supported on (1, ∞) and are 'close' to g(x).
Here we choose:$$f_1=\frac{1}{x^2},$$
$$f_2=\frac{1}{\pi}\frac{1}{1 + (x-\sqrt{2})^2}.$$
So densities and the ratio $g(x)/f(x)$ can be plotted.
```{r}
x <- seq(1, 4, .01)
w <- 2
f1 <- 1/x^2
f2 <- 1 / ((1 + (x-sqrt(2))^2) * pi)#Cauchy distribution f(x,sqrt(2),1)
g <- (x^2/sqrt(2*pi))*exp(-x^2/2)
#figure (a)
plot(x, g, type = "l", main = "", ylab = "", ylim = c(-0.5,1), lwd = w)
lines(x, f1, lty = 2, lwd = w)
lines(x, f2, lty = 3, lwd = w)
legend("topright", legend = c("g", 0:2), lty = 1:3, lwd = w, inset = 0.02)
#figure (b)
plot(x, g, type = "l", main = "", ylab = "", ylim = c(-0.5,3), lwd = w, lty = 2)
lines(x, g/f1, lty = 3, lwd = w)
lines(x, g/f2, lty = 4, lwd = w)
legend("topright", legend = c(0:2), lty = 2:4, lwd = w, inset = 0.02)
sd(g/f1)
sd(g/f2)
```
Conclusion: Standard deviation of g/f1 is smaller than g/f2, which means the ratio is closer to a constant. So f1 can be considered as a better important function.

\
5.14:
Obtain a Monte Carlo estimate of $$\int_1^\infty\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$$by importance sampling.
cdf:$$F(x)=1-\frac{1}{x}$$
```{r}
n <- 10000
u <- runif(n)
x <- 1/(1-u)
mean((x^4/sqrt(2*pi))*exp(-x^2/2))
```
We choose f1 from 5.13 as the important function to estimate this integration, and the estimation is around 0.4. 

4th

6.5: 
To obtain a Monte Carlo estimate of $\theta = E[X]$ based on 20 replicates, generate random samples $X_i$ from $\chi^2(2)$ distribution, $i=1,2,...,20.$
```{r}
n <- 50
q <- 1:200
for (k in 1:200){
  x <- rchisq(n, df = 2)
  m <- mean(x)
  m
  s <- sd(x)
  p <- m+c(-1,1*qt(0.975,n-1)*s/sqrt(n))
  a <- 1:1000
  b <- 0
  for (i in 1:1000){
    x <- rchisq(n, df = 2)
    a[i] <- mean(x)
    if (p[1]<a[i]&a[i]<p[2]){
      b <- b+1
    }
  }
  q[k] <- b/1000
}
mean(q)
UCL <- replicate(1000, expr = {
x <- rchisq(n, df = 2)
(n-1) * var(x) / qchisq(0.05, df = n-1)
})
mean(UCL>4)
```
Conclusion: The probability that the confidence interval covers the mean is larger than  the probability that the confidence interval covers the variance. So we could get the result that the t-interval is more robust to departures from normality than the interval for variance.

\
6.A:
```{r}
n <- 20
alpha <- .05
mu0 <- 1
m <- 10000 #number of replicates

p1 <- numeric(m) #storage for p-values
for (j in 1:m) {
x1 <- rchisq(n, df=1)
ttest <- t.test(x1,alternative = "two.sided", mu = mu0)
p1[j] <- ttest$p.value
}
p.hat1 <- mean(p1 < alpha)
se.hat1 <- sqrt(p.hat1 * (1 - p.hat1) / m)
print(c(p.hat1, se.hat1))

p2 <- numeric(m) #storage for p-values
for (j in 1:m) {
x2 <- runif(n, 0,2)
ttest <- t.test(x2,alternative = "two.sided", mu = mu0)
p2[j] <- ttest$p.value
}
p.hat2 <- mean(p2 < alpha)
se.hat2 <- sqrt(p.hat2 * (1 - p.hat2) / m)
print(c(p.hat2, se.hat2))

p3 <- numeric(m) #storage for p-values
for (j in 1:m) {
x3 <- rexp(n,1)
ttest <- t.test(x3,alternative = "two.sided", mu = mu0)
p3[j] <- ttest$p.value
}
p.hat3 <- mean(p3 < alpha)
se.hat3 <- sqrt(p.hat3 * (1 - p.hat3) / m)
print(c(p.hat3, se.hat3))

Type_I_error<-c(p.hat1,p.hat2,p.hat3)
sd_Type_I_error<-c(se.hat1,se.hat2,se.hat3)
Three_Distribution<-c("Chisq","Uniform","Exp")
MC_result<-data.frame(Three_Distribution,Type_I_error,sd_Type_I_error)
MC_result
```

\
class:

(1)$H_0:\beta_1-\beta_2=0$; $H_1:\beta_1-\beta_2\neq 0$. Take $\alpha=0.05.$

(2)Paired-t test is better. Note the result of test 1\(X_i\), the result of test 1\(Y_i\), correct rejection is 1 and wrong rejection is 0. \(X_i,Y_i\in\{0,1\},i=1,\cdots,n\), \(\frac{1}{n}\sum_{i=1}^n(Y_i-X_i)\sim N(\beta_1-\beta_2,\frac{\sigma^2}{n})\),\(\hat{\sigma_n}^2=\sum_{i=1}^n(Y_i-X_i-\bar{Y}-\bar{X})^2/n-1\). So\(\frac{1/n\sum_{i=1}^n(Y_i-X_i)}{\sqrt{\hat{\sigma_n}^2/n}}\sim t(n-1)\) .It can calculate the difference between paired samples.

(3)\(t_{n-1,\alpha/2}\leq T< t_{n-1,1-\alpha/2}\), so we only need to know the quantile of t distribution and the distance between two probabilities.

5th

6.C: 
Here we take $d=3$, $\alpha=0.05$ and $n=10,20,30,50,100,500$ for Type I error in Mardia’s multivariate skewness test; Then take $d=3$, $\alpha=0.1$ and $n=30$ for Type II error(power).
```{r}
d <- 2
#Type I error
n <- c(10, 20, 30, 50, 100, 500) #sample sizes
cv <- (6/n)*qchisq(.95, d*(d+1)*(d+2)/6) 

sk <- function(x,n) {
#computes the sample skewness coeff.
xbar <- apply(x,2,mean)
x2 <- matrix(nr=n,nc=n) 
for(i in 1:n){
  x1 <- (x[i,]-xbar)%*%solve(cov(x)/(n-1)*n)
  for(j in 1:n){
    x2[i,j] <- x1%*%(x[j,]-xbar)
  }
}
return(mean(x2^3))
}

p.reject <- numeric(length(n)) #to store sim. results
m <- 200 #num. repl. each sim.
for (i in 1:length(n)) {
  sktests <- numeric(m) #test decisions
  for (j in 1:m) {
    x <- matrix(rnorm(n[i]*d),nr=n[i],nc=d)
#test decision is 1 (reject) or 0
    sktests[j] <- as.integer(sk(x,n[i]) >= cv[i] )
  }
  p.reject[i] <- mean(sktests) #proportion rejected
}
p.reject
```
Conclusion1: When the sum of samples increases, the estimates of p-value can be closer to $\alpha=0.05$.
```{r}
#Type II error
alpha <- .1
n <- 30
m <- 200
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- (6/n)*qchisq(1-alpha, d*(d+1)*(d+2)/6) 
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
for (i in 1:m) { #for each replicate
  sigma <- sample(c(1, 10), replace = TRUE, size = n, prob = c(1-e, e))
  x <- matrix(rnorm(n*d, 0, sigma),nr=n,nc=d)
  sktests[i] <- as.integer(sk(x,n) >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
pwr
```
Conclusion2: When $\epsilon falls around (0.1,0.3)$, the power function can be very high.

6th

```{r}
library(boot)
data(scor,package = "bootstrap")
n <- nrow(scor)
mec <- scor$mec
vec <- scor$vec
alg <- scor$alg
ana <- scor$ana
sta <- scor$sta
x <- cbind(mec,vec,alg,ana,sta)
a <- eigen(cov(scor))
theta.hat <- max(a$val)/sum(a$val)
theta.hat

#jackknife
theta.jack <- numeric(n)
for (i in 1:n){
  scor_new <- cbind(mec[-i],vec[-i],alg[-i],ana[-i],sta[-i])
  a_new <- eigen(cov(scor_new))
  theta.jack[i] <- max(a_new$val)/sum(a_new$val)
}
bias <- (n-1)*(mean(theta.jack)-theta.hat)
se <- sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))
bias
se

#bootstrap
B <- 200
R <- numeric(B)
for(b in 1:B){
  i <- sample(1:n,size = n, replace = TRUE)
  mec <- scor$mec[i]
  vec <- scor$vec[i]
  alg <- scor$alg[i]
  ana <- scor$ana[i]
  sta <- scor$sta[i]
  scor_new0 <- cbind(mec,vec,alg,ana,sta)
  a_new0 <- eigen(cov(scor_new0))
  R[b] <- max(a_new0$val)/sum(a_new0$val)
}
bias.R <- mean(R-theta.hat)
bias.R
se.R <- sd(R)
se.R

eigenmax <- function(x,i){
  scor_new1 <- cbind(x[i,1],x[i,2],x[i,3],x[i,4],x[i,5])
  a_new1 <- eigen(cov(scor_new1))
  max(a_new1$val)/sum(a_new1$val)
}
obj <- boot(data = scor, statistic = eigenmax, R = 2000)
obj

#95%percentile
boot.ci(obj, type = c("basic", "norm", "perc"))

#BCa
boot.BCa <-
function(x, th0, th, stat, conf = .95) {
# bootstrap with BCa bootstrap confidence interval
# th0 is the observed statistic
# th is the vector of bootstrap replicates
# stat is the function to compute the statistic
  x <- as.matrix(x)
  n <- nrow(x) #observations in rows
  N <- 1:n
  alpha <- (1 + c(-conf, conf))/2
  zalpha <- qnorm(alpha)
  # the bias correction factor
  z0 <- qnorm(sum(th < th0) / length(th))
  # the acceleration factor (jackknife est.)
  th.jack <- numeric(n)
  for (i in 1:n) {
    J <- N[1:(n-1)]
    th.jack[i] <- stat(x[-i, ], J)
  }
  L <- mean(th.jack) - th.jack
  a <- sum(L^3)/(6 * sum(L^2)^1.5)
  # BCa conf. limits
  adj.alpha <- pnorm(z0 + (z0+zalpha)/(1-a*(z0+zalpha)))
  limits <- quantile(th, adj.alpha, type=6)
  return(list("est"=th0, "BCa"=limits))
}
boot.BCa(x, th0 = theta.hat, th = R, stat = eigenmax)
```

```{r}
library(moments)
m<-100
B<-100
n<-50
SK<-function(x,i){
  skewness(x[i])
}
ci_basic_No<-ci_percent_No<-matrix(NA,m,2)
for (i in 1:m) {
  dt<-rnorm(n,mean = 0,sd=1)
  de<-boot(data=dt,statistic=SK,R=B)
  ci<-boot.ci(de,type=c("basic","perc"))
  ci_basic_No[i,]<-ci$basic[4:5]
  ci_percent_No[i,]<-ci$perc[4:5]
}
ci_basic_Chi<-ci_percent_Chi<-matrix(NA,m,2)
for (i in 1:m) {
  dt<-rchisq(n,5)
  de<-boot(data=dt,statistic=SK,R=B)
  ci<-boot.ci(de,type=c("basic","perc"))
  ci_basic_Chi[i,]<-ci$basic[4:5]
  ci_percent_Chi[i,]<-ci$perc[4:5]
}

r_No_Basic<-mean(ci_basic_No[,1]<0 & ci_basic_No[,2]>0)
r_No_percent<-mean(ci_percent_No[,1]<0 & ci_percent_No[,2]>0)
r_Chi_Basic<-mean(ci_basic_Chi[,1]>0)
r_Chi_percent<-mean(ci_percent_Chi[,1]>0)

Lable1<-c("r_No_Basic","r_No_percent","r_Chi_Basic","r_Chi_percent")
res1<-c(r_No_Basic,r_No_percent,r_Chi_Basic,r_Chi_percent)
re1<-data.frame(Lable1,res1)
re1

r1_NL <- mean(ci_basic_No[,1] > 0)
r1_NR <- mean(ci_basic_No[,2] < 0)
r2_NL <- mean(ci_percent_No[,1] > 0)
r2_NR <- mean(ci_percent_No[,2] < 0)
r1_ChiL <- mean(ci_basic_Chi[,1] < 0)
r2_ChiL <- mean(ci_percent_Chi[,1] < 0)

Lable2 <- c("NoSK_Left_Basic", "NoSK_Right_Basic", "NoSK_Left_percent", "NoSK_Right_percent", "ChiSK_Left_Basic", "ChiSK_Left_percent")
res2 <- c(r1_NL, r1_NR, r2_NL, r2_NR, r1_ChiL, r2_ChiL)
re2 <- data.frame(Lable2, res2)
re2
```

7th

(1)
8.2: Implement the bivariate Spearman rank correlation test for independence as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method  "spearman". Compare the achieved significance level of the permutation test with the p-value got by cor.test on the same samples.

```{r}
dt <- matrix(data=NA, nrow = 100, ncol = 2)
x <- dt[1:100, 1] <- rnorm(100, 0, 1)
y <- dt[1:100, 2] <- rnorm(100, 0, 1)
cor_res <- cor.test(x, y, method = "spearman")
cor0 <- cor_res$statistic
R <- 999 #number of replicates
z <- c(x, y) #pooled sample
K <- 1:200
reps <- numeric(R) #storage for replicates
for (i in 1:R) {
#generate indices k for the first sample
  k <- sample(K, size = 100, replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  reps[i] <- cor.test(x1, y1, method = "spearman")$statistic
}
p_hat <- mean(c(cor0, reps) >= cor0)
cor_res
p_hat
```
If p_hat is less than 0.5, two times of p_hat(ASL) is closed to the p-value reported by cor.test; if p_hat is larger than 0.5, two times of 1-p_hat(ASL) is closed to the p-value reported by cor.test.

(2)
Design experiments for evaluating the performance of the NN,
energy, and ball methods in various situations.

Unequal variances and equal expectations;
Unequal variances and unequal expectations;
Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions);
Unbalanced samples (say, 1 case versus 10 controls).
Note: The parameters should be chosen such that the powers
are distinguishable (say, range from 0.3 to 0.8).
```{r}
m <- 100
#unequal var & equal E
x <- rnorm(50, 0, 1)
y <- rnorm(50, 0, 2)
library(RANN) # implementing a fast algorithm
# for locating nearest neighbors
# (alternative R package: "yaImpute")
library(boot)
z <- c(x, y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]
  n2 <- sizes[2]
  n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ]
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5)
  i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
N <- c(length(x), length(y))
NN <- function(z, N, k){
  boot.obj <- boot(data = z, statistic = Tn, R = 99,
  sim = "permutation", sizes = N,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p <- mean(ts>=ts[1])
  p
}
p.value <- NN(z, N, 3)
p.value
library(energy)
boot.obs <- eqdist.etest(z, sizes=N, R=99)
p.value1 <- boot.obs$p.value
p.value1
library(Ball)
p.value2 <- bd.test(x = x, y = y, num.permutations=99)$p.value
p.value2
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- rnorm(50, 0, 1)
  y <- rnorm(50, 0, 2)
  z <- c(x, y)
  p.values[i,1] <- NN(z, N, 3)
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=99)$p.value
}
pow <- colMeans(p.values<0.05)
pow

#unequal var & unequal E
x <- rnorm(50, 0, 1)
y <- rnorm(50, 1, 2)
z <- c(x, y)
N <- c(length(x), length(y))
p.value <- NN(z, N, 3)
p.value
boot.obs <- eqdist.etest(z, sizes=N, R=99)
p.value1 <- boot.obs$p.value
p.value1
p.value2 <- bd.test(x = x, y = y, num.permutations=99)$p.value
p.value2
for(i in 1:m){
  x <- rnorm(50, 0, 1)
  y <- rnorm(50, 1, 2)
  z <- c(x, y)
  p.values[i,1] <- NN(z, N, 3)
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=99)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=99)$p.value
}
pow <- colMeans(p.values<0.05)
pow

#Non-normal distributions
x <- rt(50, 1)
y <- 0.3*rnorm(50, 0, 1) + 0.7*rnorm(50, 1, 3)
z <- c(x, y)
N <- c(length(x), length(y))
p.value <- NN(z, N, 3)
p.value
boot.obs <- eqdist.etest(z, sizes=N, R=99)
p.value1 <- boot.obs$p.value
p.value1
p.value2 <- bd.test(x = x, y = y, num.permutations=99)$p.value
p.value2
for(i in 1:m){
  x <- rt(50, 1)
  y <- 0.3*rnorm(50, 0, 1) + 0.7*rnorm(50, 1, 3)
  z <- c(x, y)
  p.values[i,1] <- NN(z, N, 3)
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=99)$p.value
}
pow <- colMeans(p.values<0.05)
pow

#Unbalanced samples
x <- rnorm(20, 0, 1)
y <- rnorm(100, 1, 1)
z <- c(x, y)
N <- c(length(x), length(y))
p.value <- NN(z, N, 3)
p.value
boot.obs <- eqdist.etest(z, sizes=N, R=99)
p.value1 <- boot.obs$p.value
p.value1
p.value2 <- bd.test(x = x, y = y, num.permutations=99)$p.value
p.value2
for(i in 1:m){
  x <- rnorm(20, 0, 1)
  y <- rnorm(100, 1, 1)
  z <- c(x, y)
  p.values[i,1] <- NN(z, N, 3)
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,num.permutations=99)$p.value
}
pow <- colMeans(p.values<0.05)
pow
```
Conclusion:
1. Ball method is the most powerful test when variance is different or both variance and expectation are different.
2. From these four situations, we could see that energy and ball methods are both more powerful than NN method. But the former two methods cannot be uniformly distinguished. Moreover, if these two methods need to be distinguished, we should discuss in different situations.

8th

## Answer 9.3

Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution.

$Cauchy(\theta=1,\eta=0)$ distribution:
$$f(x)=\frac{1}{\pi(1+x^2)}.$$
```{r}
# # data generation
# m <- 10000
# x <- numeric(m)
# x[1] <- rnorm(1,10)
# k <- 0
# u <- runif(m)
# for (i in 2:m) {
#   xt <- x[i-1]
#   y <- rnorm(1,xt)
#   num <- dcauchy(y) * dnorm(xt, y)
#   den <- dcauchy(xt) * dnorm(y, xt)
#   if (u[i] <= num/den) 
#     x[i] <- y 
#   else {
#     x[i] <- xt
#     k <- k+1 #y is rejected
#   }
# }
# print(k)/m
# index <- 1000:5000
# y1 <- x[index]
# plot(index, y1, type="l", main="", ylab="x")
```

Cdf of Cauchy distribution is:
$$F(x)=\frac{1}{\pi}{\rm arctan}x+\frac{1}{2}$$

So  an explicit formula for the quantiles of Cauchy distribution are given by
$$x_q=F^{-1}(q)={\rm tan}[(q-\frac{1}{2})\pi]$$

```{r}
# b <- 1000 #discard the burning sample
# y <- x[b:m]
# a <- seq(0.1, 0.9, 0.1)
# QR <- tan((a-0.5)*pi) #deciles of Cauchy
# Q <- quantile(x, a)
# qqplot(QR, Q, main="", xlab="Cauchy deciles", ylab="Sample deciles")
# y <- x <- seq(-4, 3, 0.01)
# lines(x,y)
```

From the plot, it appears that the sample deciles are in approximate agreement with the theoretical deciles (the points fall around the line y=x). 

## Answer 9.8

Fixed a=10, b=10, n=20, the conditional distribution are ${\rm Binomial}(20,y)$ and ${\rm Beta}(x+10,30-x).$

$$P(X=x|Y=y)=C_{20}^xy^x(1-y)^{20-x}$$
$$f(y|x)=\frac{\Gamma (40)}{\Gamma (x+10)\Gamma (30-x)}y^{x+9}(1-y)^{29-x}$$

```{r}
gib.chain <-function(N,a1,a2){
  X <- matrix(0, N, 2)
  X[1,]<-c(a1,a2)
  for(i in 2:N){
    X[i,1] <- rbinom(1, 20, X[i-1,2])
    X[i,2] <- rbeta(1, X[i,1]+10, 30-X[i,1])
  }
  return(X)
}
# N <- 5000
# b <- 1000
# gib.chain(N, 0, 0)[(b+1):N,]
```

## Answer added question

Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R<1.2$.
```{r}
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}
#9.3 Gelman.Rubin
cauchy.chain <- function(N, x0) {
  x <- rep(0, N)
  x[1] <- x0
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rnorm(1,xt,1)#candidate point
    r1 <- dcauchy(y) * dnorm(xt, y, 1)
    r2 <- dcauchy(xt) * dnorm(y, xt, 1)
    r <- r1 / r2
    if (u[i] <= r) 
      x[i] <- y 
    else
      x[i] <- xt
  }
  return(x)
}
# k <- 4 #number of chains to generate
# n <- 15000 #length of chains
# b <- 1000 #burn-in length
# #choose overdispersed initial values
# x0 <- c(-10, -5, 5, 10)
# #generate the chains
# Y <- matrix(0, nrow=k, ncol=n)
# for (i in 1:k)
#   Y[i, ] <- cauchy.chain(n, x0[i])
# #compute diagnostic statistics
# psi <- t(apply(Y, 1, cumsum))
# for (i in 1:nrow(psi))
#   psi[i,] <- psi[i,] / (1:ncol(psi))
# print(Gelman.Rubin(psi))
# #plot psi for the four chains
# plot(psi[1, (b+1):n], type="l")
# plot(psi[2, (b+1):n], type="l")
# plot(psi[3, (b+1):n], type="l")
# plot(psi[4, (b+1):n], type="l")
# rhat <- rep(0, n)
# for (j in (b+1):n)
#   rhat[j] <- Gelman.Rubin(psi[,1:j])
# plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
# 
# #9.8 Gelman.Rubin
# k <- 4
# n <- 15000
# b <- 1000
# #generate the chains
# X1 <- X2 <- matrix(0, nrow=k, ncol=n)
# for (i in 1:k){
#   X1[i, ] <- gib.chain(n, 0, 0)[,1]
#   X2[i, ] <- gib.chain(n, 0, 0)[,2]
# }
# #compute diagnostic statistics
# psi1 <- t(apply(X1, 1, cumsum))
# psi2 <- t(apply(X2, 1, cumsum))
# for (i in 1:nrow(psi)){
#   psi1[i,] <- psi1[i,] / (1:ncol(psi1))
#   psi2[i,] <- psi2[i,] / (1:ncol(psi2))
# }
# print(Gelman.Rubin(psi1))
# print(Gelman.Rubin(psi2))
# #plot psi for the four chains
# rhat1 <- rhat2 <- rep(0, n)
# for (j in (b+1):n){
#   rhat1[j] <- Gelman.Rubin(psi1[,1:j])
#   rhat2[j] <- Gelman.Rubin(psi2[,1:j])
# }
# plot(rhat1[(b+1):n], type="l", xlab="", ylab="R", main="x")
# plot(rhat2[(b+1):n], type="l", xlab="", ylab="R", main="y")
```

In both exercises, all chains meet the requirements for convergences: $\hat R<1.2$.

9th

11.3
To compute the $k^{{\rm th}}$ term in the summation formula and return the sum.
```{r}
f_1 <- function(k,a,d){
  y <- (-1/2)^k/prod(1:k)*norm(a,"2")^(2*k+2)/((2*k+1)*(2*k+2))*gamma((d+1)/2)*gamma(k+3/2)/gamma(k+d/2+1)
  return(y)
}

f1_sum <- function(a,d){
  sum <- 0
  k <- 1
  y <- f_1(k,a,d)
  while(abs(y) > 1e-5){# condition for convergence (error)
    sum <- sum+y
    k <- k+1
    y <- f_1(k,a,d)
  }
  return(list(sum,k))
}

k_a <- f_1(10,c(1,2),2)# the kth of this sum
k_a
sum_a <- f1_sum(c(1,2),2)# print out the sum and k when it converges(error<1e-5).
sum_a
```

11.5

```{r}
# 11.4/11.5
# s_k(a)
sk1 <- function(a,k){
  return(1-pt(sqrt(a^2*(k-1)/(k-a^2)),k-1))
}
k <- c(seq(4,25,1),100,500,1000)
res1 <- res2 <- length(k)
for (i in 1:length(k)) {
  a <- seq(0,sqrt(k[i]),0.005)
  p1 <- p2 <- numeric(length(a))
  for (j in 1:length(a)) {
    p1[j]<-sk1(a[j],k[i])
    p2[j]<-sk1(a[j],k[i]+1)
  }
  # the best choice for approximate equality and get its index
  index_min <- which.min(abs(p2[-1]-p1[-1]))
  res1[i] <- a[-1][index_min]
}
# two sides of the equation in 11.5 are pdf of t distribution with (k-1) and k degrees of freedom
for (i in 1:length(k)) {
  sk2 <- function(a) {
    int1 <- pt(sqrt(a^2*(k[i]-1)/(k[i]-a^2)),k[i]-1)
    int2 <- pt(sqrt(a^2*(k[i])/(k[i]+1-a^2)),k[i])
    return(int1-int2)
  }
  res2[i] <- uniroot(sk2,c(0.0001,sqrt(k[i])-0.0001))$root
}
res1
res2
error <- abs(res1-res2)
res <-as.data.frame(cbind(res1,res2,error))
res
```
Conclusion: 
1. When $k\leq 25$, the distance between 11.4 and 11.5 is not significant;
2. The results in 11.4 and 11.5 differ much more from each other when $k$ increases($k\geq 100$).

## Answer added exercise -- EM algorithm
The number of samples meeting $T_i>1$ is 3, and the number of samples meeting $T_i\leq1$ is 7. So we let the complete data:$(y_1,...,y_7,z_1,z_2,z_3),y=(y_1,...,y_7),z=(z_1,z_2,z_3)$, $z$ is unknown.

$$L_c(\lambda)=\lambda^{10}\exp\{-\lambda(\sum_{i=1}^7y_i+\sum_{j=1}^3z_j)\}$$
E step: 
$$\mathbb{E}[z_j|\hat{\lambda^{(i)}}]=\mathbb{E}[z_j]/\mathbb{P}(\hat{\lambda^{(i)}})=\int_1^\infty\lambda z_i(\exp(-\lambda z_i))\mathrm{d}z_i/\exp(-\lambda )=1+\frac{1}{\hat{\lambda}^{(i)}}$$
$$\mathbb{E}[l_c(\lambda)]=10\ln\lambda - \lambda(\sum_{i=1}^7y_i+\sum_{j=1}^3\mathbb{E}[z_j|\hat{\lambda^{(i)}}])$$
$$\frac{\mathrm{d}\mathbb{E}[l_c(\lambda)]}{\mathrm{d\lambda}}=\left[\frac{10}{\lambda}-(\sum_{i=1}^7y_i+\sum_{j=1}^3\mathbb{E}[z_j|\hat{\lambda^{(i)}}])\right]=0$$
So we can get:
$$\hat{\lambda}^{(i+1)}=\frac{10}{\sum_{i=1}^7y_i+\sum_{j=1}^3\mathbb{E}z_j}=\frac{10}{\sum_{i=1}^7y_i+3(1+\frac{1}{\hat{\lambda}^{(i)}})}.$$
The likelihood function of observed data is
$$L_o(\lambda)=\lambda^7\exp\{-\lambda\sum_{i=1}^7y_i\}\Pi_{i=1}^{3}\exp(-\lambda)$$
$$l_0(\lambda)=7\ln\lambda -\lambda\sum_{i=1}^7y_i-3\lambda.$$
So we can get:$$\lambda=\frac{7}{\sum_{i=1}^7y_i+3}.$$
```{r}
m <- 200
lambda <- numeric(m)
lambda[1] <- 1
y <- c(0.54,0.48,0.33,0.43,1,1,0.91,1,0.21,0.85)
for(i in 2:m){
  lambda[i] <- 10/(sum(y)+3/lambda[i-1])
}
lambda_0 <- 7/sum(y)
lambda[m]
lambda_0
```
So we can easily get the conclusion that value of $\lambda$ got from EM algorithm converges to the true $\lambda$.

10th

1.1

Discuss the reason why these two invocations of lapply() equivalent.
```{r}
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
lapply(trims, function(trim) mean(x, trim = trim))
lapply(trims, mean, x = x)
```
Why are the following two invocations of lapply() equivalent?

1. We can use an anonymous function or a new named function or a built-in function.

2. lapply() can take a name of an function searched from match.fun, so `mean` function works(the second line). Then lapply() can apply `mean` to `trims`. 

3. For the second line, by `x = x`, lapply() knows that the x is the variable for `mean` function, and then passes `trims` as the second argument to `mean`.

1.5

For each model in 1.3 and 1.4, extract $R^2$ using:

rsq <- function(mod) summary(mod)$r.squared

```{r}
# 1.3
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
# With a for loop
out_1 <- vector("list", length(formulas))
i <- 1
for (formula in formulas) { 
  out_1[[i]] <- lm(formula, data = mtcars)
  i <- i + 1 
}
# With lapply
out_2 <- lapply(formulas, lm, data = mtcars)

# 1.4
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
# With a for loop
out_3 <- vector("list", length(bootstraps))  
i <- 1
for (bootstrap in bootstraps) {
  out_3[[i]] <- lm(mpg ~ disp, data = bootstrap)
  i <- i + 1
}
# With lapply
out_4 <- lapply(bootstraps, lm, formula = mpg ~ disp)

# 1.5
rsq <- function(mod) summary(mod)$r.squared
outs <- list(out_1, out_2, out_3, out_4)
rsqs <- function(out) lapply(out, rsq)
lapply(outs, rsqs)
```

2.1

Use vapply() to compute the standard deviation of every column in a numeric data frame
and to compute the standard deviation of every numeric column in a mixed data frame.

```{r}
vapply(mtcars, sd, numeric(1)) # numeric data frame
sd0 <- function(data) vapply(data[vapply(data, is.numeric, logical(1))], sd, numeric(1))
sd0(USJudgeRatings) # mixed data frame
```

2.7

Implement mcsapply(), a multicore version of sapply(). Can mcvapply() be implemented?
```{r}
# mcapply()
mcsapply<-function(x,fun){
library(parallel)
  cl <- makeCluster(3)
  f <- parSapply(cl, x, fun)
  stopCluster(cl)
  return(f)
}
simulation <- replicate(500,
t.test(rnorm(10, 5, 10), rcauchy(10, 5)), simplify = FALSE)
list(system.time(mcsapply(simulation, function(x) {
  unlist(x)[3]
})),system.time(sapply(simulation, function(x) {
  unlist(x)[3]
})))
```
We can see computing speed gets great increased, so I think the mcvapply() can be implemented as mcsapply() does.

11th

## Answer 1
```{r}
# R code
# gib.chain <- function(a, b, n){
#   N <- 5000
#   burn <- 1000
#   X <- matrix(0, N, 2)
#   X[1,]<-c(0, 0)
#   for(i in 2:N){
#     X[i, 1] <- rbinom(1, n, X[i-1, 2])
#     X[i, 2] <- rbeta(1, X[i, 1] + a, n + b - X[i, 1])
#   }
#   x <- X[(burn + 1):N, ]
#   return(x)
# }
# C++ code (Rcpp function)
library(Rcpp)
cppFunction('NumericMatrix gib_chain_rcpp(double a, double b, double m) {
  int N = 5000;
  int burn = 1000;
  NumericMatrix X(N, 2);
  NumericVector v = {0, 0};
  X(0, _) = v;
  for (int i = 1; i < N; i++) {
    double x1 = X(i - 1, 1);
    X(i, 0) = as<int>(rbinom(1, m, x1));
    int x0 = X(i, 0);
    X(i, 1) = as<double>(rbeta(1, x0 + a, m + b - x0));
  }
  NumericMatrix x = X(Range(burn - 1, N - 1), Range(0, 1));
  return x;
}')
```

## Answer 2
```{r}
# qqplot
a <- 10
b <- 10
n <- 20
X_R <- gib.chain(a, b, n)
X_C <- gib_chain_rcpp(a, b, n)
qqplot(X_R[, 1], X_C[, 1], xlab = "Discrete_R", ylab = "Discrete_Rcpp")
qqplot(X_R[, 2], X_C[, 2], xlab = "Continuous_R", ylab = "Continuous_Rcpp")
```

## Answer 3
```{r}
library(microbenchmark)
ts <- microbenchmark(ts1 <- gib.chain(a, b, n), ts2 <- gib_chain_rcpp(a, b, n))
ts
```

## Answer 4
Comments for result:
Rcpp function can perform as R function does, but Rcpp function can be more efficient.

